{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fantasy NFL Lineup Optimizer\n",
    "First attempt at getting a working model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using POMDPModelTools\n",
    "using Random\n",
    "using Distributions\n",
    "using POMDPSimulators\n",
    "using POMDPPolicies\n",
    "using MCTS\n",
    "using SARSOP\n",
    "using Printf\n",
    "using CSV \n",
    "using Plots \n",
    "using DataFrames\n",
    "using LinearAlgebra\n",
    "\n",
    "rng = Random.GLOBAL_RNG; \n",
    "pyplot(); \n",
    "\n",
    "HIST_RAND_FILENAME = \"rand_results.csv\"; \n",
    "HIST_MCTS_FILENAME = \"mcts_results.csv\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Model Parameters \n",
    "\n",
    "First, try to get an extremely basic version of the model running. See SIMPLE MODEL DATA PARAMS for details \n",
    "\n",
    "### Real Model Data\n",
    "Need to write function to read in arrays of all the load/solar, occupancy, etc. from CSV files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States, Actions, and MDP Definition\n",
    "- Data containers representing the state and actions of the FantasyGame\n",
    "- MDP data container holds all the information needed to define MDP tuple (S,A,T,R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct FantasyGameState\n",
    "    proj::Array{Float64}\n",
    "    sal::Array{Float64}\n",
    "    pos::Array{Float64} \n",
    "    team::Array{String} \n",
    "    inj::Array{String} # Should convert the transition function to sampling from Dirichlet\n",
    "    week::Int64\n",
    "end\n",
    "\n",
    "struct FantasyGameAction\n",
    "    lineup::Array{Bool}\n",
    "end\n",
    "\n",
    "struct FantasyGameMDP <: MDP{FantasyGameState, FantasyGameAction}\n",
    "    # Define DFS FantasyGameMDP \n",
    "    \n",
    "    # Roster Constrain Params \n",
    "    rb_max::Int64\n",
    "    wr_max::Int64\n",
    "    qb_max::Int64\n",
    "    te_max::Int64\n",
    "    sal_max::Int64\n",
    "end \n",
    "\n",
    "RB_MAX = 2; \n",
    "WR_MAX = 2;\n",
    "QB_MAX = 1;\n",
    "TE_MAX = 1;\n",
    "SAL_MAX = 60000; # $60K \n",
    "\n",
    "FantasyGameMDP() = FantasyGameMDP(RB_MAX, WR_MAX, QB_MAX, TE_MAX, SAL_MAX)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define gen \n",
    "Implement the complete generative models for both the FantasyGameMDP and FantasyGamePOMDP\n",
    "- State transition model \n",
    "- Observation model \n",
    "- Reward model \n",
    "\n",
    "LOTS of work still needed to refine these... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In theory, none of these state transition functions should impact policy \n",
    "# because the reward is pretty independent of what the state transition is... \n",
    "# Like, the action of our agent has no impact on the next state, so yeah... \n",
    "function update_proj(proj) \n",
    "    # Add random step sampled from normal dist each week. (this shouldn't impact policy) \n",
    "    proj_next = proj + rand(Normal(0,0.05*proj),1) \n",
    "    proj_next = max(proj_next, 0) \n",
    "    return proj_next \n",
    "end\n",
    "\n",
    "function update_sal(sal) \n",
    "    # Add random step sampled from normal dist each week. (this shouldn't impact policy) \n",
    "    sal_next = sal + rand(Normal(0,0.05*sal),1) \n",
    "    sal_next = max(sal_next, 0) \n",
    "    return sal_next \n",
    "end\n",
    "\n",
    "function update_week(week)\n",
    "    return week+1\n",
    "end\n",
    "\n",
    "# TODO: make the update injury function work with arrayed input \n",
    "function update_inj(inj)\n",
    "    if false # Using this to mask the following computations until it can be re-written for array input \n",
    "        heal_prob = 0.3 \n",
    "        inj_prob = 0.03 \n",
    "        if inj != 0 \n",
    "            if rand(Binomial(1,heal_prob),1) == 1\n",
    "                inj = 0 \n",
    "            end\n",
    "        else\n",
    "            if rand(Binomial(1,inj_prob),1) == 1\n",
    "                inj = \"Q\" \n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return inj\n",
    "end\n",
    "\n",
    "function update_team(team) \n",
    "    return team \n",
    "end\n",
    "\n",
    "function update_pos(pos) \n",
    "    return pos \n",
    "end\n",
    "\n",
    "# MDP Generative Model \n",
    "function POMDPs.gen(m::FantasyGameMDP, s::FantasyGameState, a::FantasyGameAction, rng)\n",
    "    # Transition Model \n",
    "    week = update_week(s.week) \n",
    "    proj = update_proj(s.proj) \n",
    "    sal = update_sal(s.sal) \n",
    "    inj = update_sal(s.inj) \n",
    "    team = update_team(s.team) \n",
    "    pos = update_pos(s.pos) \n",
    "    \n",
    "    sp = FantasyGameState(proj, sal, pos, team, inj, week) \n",
    "    \n",
    "    # Observation Model \n",
    "    # N/A \n",
    "    \n",
    "    # Reward Model \n",
    "    r = dot(a.lineup, s.proj) # Raw Projected Score \n",
    "    r += count_te(a.lineup, s.pos) > m.te_max ? m.lineup_penalty : 0 \n",
    "    r += count_qb(a.lineup, s.pos) > m.qb_max ? m.lineup_penalty : 0 \n",
    "    r += count_rb(a.lineup, s.pos) > m.rb_max ? m.lineup_penalty : 0 \n",
    "    r += count_wr(a.lineup, s.pos) > m.wr_max ? m.lineup_penalty : 0 \n",
    "    r += dot(a.lineup, s.sal) > m.sal_max ? m.lineup_penalty : 0 \n",
    "    \n",
    "    # create and return a NamedTuple \n",
    "    return (sp=sp, r=r) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: FantasyGameMDP not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: FantasyGameMDP not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[1]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "fg = FantasyGameMDP(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Through Random Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "POMDPs.initialstate(m::SmartHomeMDP, rng::MersenneTwister) = SmartHomeState(5, 5, 5, 5, true, 4, 6, 1, 5, 2, 1)  \n",
    "POMDPs.initialstate_distribution(m::SmartHomeMDP) = SparseCat([SmartHomeState(5, 5, 5, 5, true, 4, 6, 1, 5, 2, 1), SmartHomeState(4, 5, 5, 5, true, 4, 6, 1, 5, 2, 1)], [0.4, 0.6])\n",
    "\n",
    "# TODO: Enumerate more actions \n",
    "POMDPs.actions(m::SmartHomeMDP) = [\n",
    "    SmartHomeAction(-C_RATE_MAX ,0,0),            SmartHomeAction(0,0,0),            SmartHomeAction(C_RATE_MAX,0,0), \n",
    "    SmartHomeAction(-C_RATE_MAX ,SP_ADJ_SIZE,0),  SmartHomeAction(0,SP_ADJ_SIZE,0),  SmartHomeAction(C_RATE_MAX,SP_ADJ_SIZE,0), \n",
    "    SmartHomeAction(-C_RATE_MAX ,0,SP_ADJ_SIZE),  SmartHomeAction(0,0,SP_ADJ_SIZE),  SmartHomeAction(C_RATE_MAX,0,SP_ADJ_SIZE), \n",
    "    SmartHomeAction(-C_RATE_MAX ,-SP_ADJ_SIZE,0), SmartHomeAction(0,-SP_ADJ_SIZE,0), SmartHomeAction(C_RATE_MAX,-SP_ADJ_SIZE,0), \n",
    "    SmartHomeAction(-C_RATE_MAX ,0,-SP_ADJ_SIZE), SmartHomeAction(0,0,-SP_ADJ_SIZE), SmartHomeAction(C_RATE_MAX,1,-SP_ADJ_SIZE)]\n",
    "\n",
    "POMDPs.discount(m::SmartHomeMDP) = DISCOUNT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_policy = RandomPolicy(sh)\n",
    "iter = 1 \n",
    "for (s, a, r) in stepthrough(sh, rand_policy, \"s,a,r\", max_steps=100)\n",
    "    if iter < SIM_DURATION\n",
    "        println(string(\"TOD: \", s.tod, \", SOC: \", s.soc, \", OCC: \", s.occ, \", ODT: \", s.odt, \", HSP/CSP: \", s.hsp, \"/\", s.csp)) \n",
    "    end\n",
    "    iter += 1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve MDP \n",
    "\n",
    "Implementing on MonteCarlo Tree Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@requirements_info MCTSSolver() SmartHomeMDP()\n",
    "n_iter = 100000\n",
    "depth = TOD_RESOLUTION #* 2\n",
    "ec = 10.0\n",
    "\n",
    "solver = MCTSSolver(n_iterations=n_iter,\n",
    "    depth=depth,\n",
    "    exploration_constant=ec,\n",
    "    enable_tree_vis=true\n",
    ")\n",
    "\n",
    "\n",
    "policy = solve(solver, sh)\n",
    "state = initialstate(sh, Random.MersenneTwister(4))\n",
    "\n",
    "a = action(policy, state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using D3Trees\n",
    "D3Tree(policy, state, init_expand=2)  # click on the node to expand it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_rand = HistoryRecorder(max_steps=SIM_DURATION)\n",
    "hist_rand = simulate(hist_rand, sh, rand_policy, state)\n",
    "\n",
    "println(\"Random Policy Total discounted reward: $(discounted_reward(hist_rand))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mcts = HistoryRecorder(max_steps=SIM_DURATION)\n",
    "hist_mcts = simulate(hist_mcts, sh, policy, state)\n",
    "\n",
    "println(\"Monte Carlo Policy Total discounted reward: $(discounted_reward(hist_mcts))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Simulation Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function export_results(hist, filename)\n",
    "    # Write to file\n",
    "    open(filename, \"w\") do io\n",
    "        @printf(io, \"t,TOD,c,dhsp,dcsp,d_hv,d_,soc,rmt,occ,hsp,csp,odt,tou,r\\n\")\n",
    "        for (s, a, r, sp) in eachstep(hist, \"(s, a, r, sp)\")  \n",
    "            @printf(io, \"%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\\n\", s.t, s.tod, a.c, a.dhsp, a.dcsp, s.d_hv, s.d_, s.soc, s.rmt, s.occ, s.hsp, s.csp, s.odt, s.tou, r)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_results(hist_rand, HIST_RAND_FILENAME) \n",
    "export_results(hist_mcts, HIST_MCTS_FILENAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
