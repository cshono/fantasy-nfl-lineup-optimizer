{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:01:47.352785Z",
     "start_time": "2017-12-14T17:01:46.733813Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load in data\n",
    "Player_stats and Opp_stats files have been pulled in from the [nflgame API](http://pdoc.burntsushi.net/nflgame) via the nflgame_acquire.ipynb scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:01:48.669582Z",
     "start_time": "2017-12-14T17:01:47.356220Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fb40dcaf3c92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m player_stats_files = sorted([(int(re.sub('[^0-9]', '', f)), path + f)\n\u001b[1;32m      4\u001b[0m                              for f in files if \"player\" in f], key=lambda x: x[0])\n\u001b[1;32m      5\u001b[0m opp_stats_files = sorted([(int(re.sub('[^0-9]', '', f)), path + f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/'"
     ]
    }
   ],
   "source": [
    "path = './data/'\n",
    "files = os.listdir(path)\n",
    "player_stats_files = sorted([(int(re.sub('[^0-9]', '', f)), path + f)\n",
    "                             for f in files if \"player\" in f], key=lambda x: x[0])\n",
    "opp_stats_files = sorted([(int(re.sub('[^0-9]', '', f)), path + f)\n",
    "                          for f in files if \"opp\" in f], key=lambda x: x[0])\n",
    "\n",
    "player_dfs = {}\n",
    "for year, fn in player_stats_files:\n",
    "    df = pd.read_csv(fn, index_col=0)\n",
    "    df['team'] = df['team'].str.replace('JAX','JAC')\n",
    "    df['team'] = df['team'].str.replace('LAC','SD')\n",
    "    df['team'] = df['team'].str.replace('STL','LA')\n",
    "    del df['position_fill']\n",
    "    df['year'] = year\n",
    "    player_dfs[year] = df\n",
    "\n",
    "opp_dfs = {}\n",
    "for year, fn in opp_stats_files:\n",
    "    df = pd.read_csv(fn, index_col=0)\n",
    "    df['year'] = year\n",
    "    df['opp_TEAM'] = df['opp_TEAM'].str.replace('JAX','JAC')\n",
    "    df['opp_OPP'] = df['opp_OPP'].str.replace('JAX','JAC')\n",
    "    df['opp_TEAM'] = df['opp_TEAM'].str.replace('LAC','SD')\n",
    "    df['opp_OPP'] = df['opp_OPP'].str.replace('LAC','SD')\n",
    "    df['opp_TEAM'] = df['opp_TEAM'].str.replace('STL','LA')\n",
    "    df['opp_OPP'] = df['opp_OPP'].str.replace('STL','LA')\n",
    "    opp_dfs[year] = df\n",
    "\n",
    "new_opp_cols = ['offense', 'defense', 'opp_first_downs', 'opp_points',\n",
    "       'opp_passing_yds', 'opp_penalty_cnt', 'opp_penalty_yds', 'opp_pos_time',\n",
    "       'opp_punt_avg', 'opp_punt_cnt', 'opp_punt_yds', 'opp_rushing_yds',\n",
    "       'opp_total_yds', 'opp_turnovers', 'week', 'year']\n",
    "\n",
    "for year, df in opp_dfs.items():\n",
    "    df.columns = new_opp_cols\n",
    "    opp_dfs[year] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:01:48.677588Z",
     "start_time": "2017-12-14T17:01:48.672917Z"
    }
   },
   "outputs": [],
   "source": [
    "for year in range(2013,2018):\n",
    "    print(player_dfs[year].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean data and create target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:01:48.977323Z",
     "start_time": "2017-12-14T17:01:48.679310Z"
    }
   },
   "outputs": [],
   "source": [
    "# missing positions\n",
    "data = pd.concat(player_dfs.values())\n",
    "missing_positions = data[data.position.isnull()]\n",
    "means = missing_positions.groupby(['id','full_name'], as_index=False).mean()\n",
    "means = means[['id','full_name','passing_att', 'rushing_att', 'receiving_rec']]\n",
    "means.columns = ['id','full_name','QB','RB','WRTE']\n",
    "means = means[(means.QB != 0) | (means.RB != 0) | (means.WRTE != 0)]\n",
    "means['position_fill'] = means[['QB','RB','WRTE']].idxmax(axis=1)\n",
    "means = means[['id','position_fill']]\n",
    "means['position_fill'] = means['position_fill'].apply(lambda x: np.nan if x == 'WRTE' else x)\n",
    "# means[means.position=='WRTE'].full_name.unique()\n",
    "# missing_positions_urls = means.profile_url.unique().tolist()\n",
    "# with open('data/missing_positions_fills.pkl', 'wb') as f:\n",
    "#     pickle.dump(means, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:01:49.061978Z",
     "start_time": "2017-12-14T17:01:48.980199Z"
    }
   },
   "outputs": [],
   "source": [
    "from data.missing_plyrs import missing_WRTE\n",
    "\n",
    "def fill_positions(name):\n",
    "    \"\"\"Fill missing positions for players who are retired. The nflgame players.json \n",
    "    will not update these positions, nor are the positions available by scraping each\n",
    "    player's nfl.com profile url.\"\"\"\n",
    "    try:\n",
    "        pos = missing_WRTE[name]\n",
    "        return pos\n",
    "    except KeyError:\n",
    "        return ''\n",
    "\n",
    "def clean(player_stats, year, imputed):\n",
    "    \"\"\"Create fantasy_points (the target variable) according to a standard scoring regime.\n",
    "       Create pass/rush/reception ratios to be included in feature set for model.\n",
    "       Trim the dataset to include the four main offensive positions: QB, RB, WR, TE.\"\"\"\n",
    "    \n",
    "    player_stats['fantasy_points'] = (player_stats['passing_tds'] * 4) +\\\n",
    "    (player_stats['passing_yds'] * 0.04) +\\\n",
    "    (player_stats['passing_twoptm'] * 2) +\\\n",
    "    (player_stats['passing_ints'] * -2) +\\\n",
    "    (player_stats['rushing_tds'] * 6) +\\\n",
    "    (player_stats['rushing_yds'] * 0.1) +\\\n",
    "    (player_stats['rushing_twoptm'] * 2) +\\\n",
    "    (player_stats['receiving_tds'] * 6) +\\\n",
    "    (player_stats['receiving_yds'] * 0.1) +\\\n",
    "    (player_stats['receiving_twoptm'] * 2) +\\\n",
    "    (player_stats['kickret_tds'] * 6) +\\\n",
    "    (player_stats['puntret_tds'] * 6) +\\\n",
    "    (player_stats['fumbles_lost'] * -2)\n",
    "    \n",
    "    player_stats['passer_ratio'] = player_stats['passing_cmp']/player_stats['passing_att']\n",
    "    player_stats['PassRushRatio_Att'] = player_stats['rushing_att'] / player_stats['passing_att']\n",
    "    player_stats['PassRushRatio_Yds'] = player_stats['rushing_yds'] / player_stats['passing_yds']\n",
    "    player_stats['PassRushRatio_Tds'] = player_stats['rushing_tds'] / player_stats['passing_tds']\n",
    "    player_stats['RushRecRatio_AttRec'] = player_stats['rushing_att'] / player_stats['receiving_rec']\n",
    "    player_stats['RushRecRatio_Tds'] = player_stats['rushing_tds'] / player_stats['receiving_tds']\n",
    "    player_stats['RushRecRatio_Yds'] = player_stats['rushing_yds'] / player_stats['receiving_yds']\n",
    "    \n",
    "    player_stats = player_stats.merge(imputed, how='left', on='id')\n",
    "    player_stats['position'].fillna(player_stats['position_fill'], inplace=True)\n",
    "    player_stats['position_fill'] = player_stats['full_name'].apply(lambda x: fill_positions(x))\n",
    "    player_stats['position'].fillna(player_stats['position_fill'], inplace=True)\n",
    " \n",
    "    include_positions = ['QB', 'TE', 'WR', 'RB']\n",
    "    player_stats['position'] = player_stats['position'].str.replace('FB','RB')\n",
    "    player_stats = player_stats[player_stats['position'].isin(include_positions)]\n",
    "    return player_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:01:49.521951Z",
     "start_time": "2017-12-14T17:01:49.063329Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dictionary with keys as year of the NFL season and values \n",
    "# as a dataframe containing game summary stats for all players in that season\n",
    "player_dfs = {year:clean(df, year, means) for year, df in player_dfs.items()}\n",
    "\n",
    "for year in range(2013,2018):\n",
    "    print(player_dfs[year].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:01:49.530017Z",
     "start_time": "2017-12-14T17:01:49.523355Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are the game summary stats relevant to QB, RB, WR, and TE positions.\n",
    "\n",
    "stat_cols = ['fumbles_lost', 'fumbles_rcv', 'fumbles_tot','fumbles_trcv', 'fumbles_yds', \n",
    "       'passing_att', 'passing_cmp', 'passing_ints', 'passing_tds', 'passer_ratio',\n",
    "       'passing_twopta', 'passing_twoptm', 'passing_yds',\n",
    "       'puntret_tds','puntret_avg', 'puntret_lng', 'puntret_lngtd', 'puntret_ret',\n",
    "       'receiving_lng', 'receiving_lngtd','receiving_rec', 'receiving_tds', 'receiving_twopta',\n",
    "       'receiving_twoptm', 'receiving_yds', 'rushing_att', 'rushing_lng',\n",
    "       'rushing_lngtd', 'rushing_tds', 'rushing_twopta', 'rushing_twoptm',\n",
    "       'rushing_yds','fantasy_points',\n",
    "        'PassRushRatio_Att','PassRushRatio_Yds','PassRushRatio_Tds','RushRecRatio_AttRec',\n",
    "        'RushRecRatio_Tds','RushRecRatio_Yds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:01:50.770366Z",
     "start_time": "2017-12-14T17:01:49.533146Z"
    }
   },
   "outputs": [],
   "source": [
    "def trim_sort(df):\n",
    "    df = df.sort_values(['id','week'])\n",
    "    df = df[stat_cols+['id','week','team','position','full_name']]\n",
    "    return df\n",
    "\n",
    "def get_trend(df):\n",
    "    \n",
    "    \"\"\"Compute a three-week trend for each game statistic, for each player.\"\"\"\n",
    "    \n",
    "    # compute 3-week and 2-week points deltas\n",
    "    deltas = df.groupby(['id']).pct_change()\n",
    "    deltas = deltas.add_prefix('chg_')\n",
    "    deltas = pd.concat([df, deltas], axis=1)\n",
    "    deltas2 = deltas.groupby(['id'])[deltas.columns].shift(1).fillna(0)\n",
    "    deltas3 = deltas.groupby(['id'])[deltas.columns].shift(2).fillna(0)\n",
    "    deltas2 = deltas2.add_prefix('per2_')\n",
    "    deltas3 = deltas3.add_prefix('per3_')\n",
    "    trend_df = pd.concat([deltas, deltas2, deltas3], axis=1)\n",
    "    # average prior three deltas to get trend\n",
    "    for col in stat_cols:\n",
    "        name = 'trend_'+col\n",
    "        trend_df[name] = trend_df[['chg_'+col,'per2_chg_'+col,'per3_chg_'+col]].mean(axis=1).fillna(0)\n",
    "    return trend_df\n",
    "\n",
    "def get_cumul_mean_stats(df, weeks):\n",
    "    \n",
    "    \"\"\"Create a rolling mean for each statistic by player, by week.\"\"\"\n",
    "    \n",
    "    weeks_stats_mean = []\n",
    "    for week in weeks:\n",
    "        tmp = df[df.week <= week]\n",
    "        tmp = tmp.groupby(['id'])[stat_cols].mean().reset_index()\n",
    "        tmp = tmp.add_suffix('_mean')\n",
    "        tmp['week'] = week\n",
    "        weeks_stats_mean.append(tmp)\n",
    "    cumavg_stats = pd.concat(weeks_stats_mean)\n",
    "    cumavg_stats = cumavg_stats.rename(columns={'id_mean':'id'})\n",
    "    return cumavg_stats\n",
    "\n",
    "def get_cumul_stats_time_weighted(df, weeks):\n",
    "    \n",
    "    \"\"\"Create a rolling time-wegihted mean for each statistic by player, by week.\"\"\"\n",
    "    \n",
    "    weeks_stats_mean_wgt = []\n",
    "    for week in weeks:\n",
    "        tmp1 = df[df.week <= week]\n",
    "        mult = lambda x: np.asarray(x) * np.asarray(tmp1.week)\n",
    "        tmp = tmp1[['id']+stat_cols].set_index('id').apply(mult).reset_index()\n",
    "        tmp = tmp.groupby(['id'])[stat_cols].mean().reset_index()\n",
    "        tmp = tmp.add_suffix('_wgtmean')\n",
    "        tmp['week'] = week\n",
    "        weeks_stats_mean_wgt.append(tmp)\n",
    "    cumavg_stats_wgt = pd.concat(weeks_stats_mean_wgt)\n",
    "    cumavg_stats_wgt = cumavg_stats_wgt.rename(columns={'id_wgtmean':'id'})\n",
    "    return cumavg_stats_wgt\n",
    "\n",
    "def defensive_ptsallow(matchups, weeks, weighted=False):\n",
    "    \n",
    "    \"\"\" Compute the mean weekly points given up by each defense to each position.\n",
    "        Parameters:\n",
    "                    matchups: dataframe of matchups between offensive player, and\n",
    "                              defensive opponent.\n",
    "                    weeks:    list of weeks in the season.\n",
    "                    weighted: boolean. If true, compute weekly points allowed according\n",
    "                              to player-weighted fantasy points.\n",
    "    \"\"\"\n",
    "    \n",
    "    agg_col = 'fantasy_points'\n",
    "    output_name = 'defensive_matchup_allowed'\n",
    "    if weighted:\n",
    "        agg_col = 'weighted_fantasy_points'\n",
    "        output_name = 'defensive_matchup_allowed_wgt'\n",
    "    # compute weekly cumulative mean points allowed by each defense\n",
    "    defense_ranks_dfs = []\n",
    "    for week in weeks:\n",
    "        matchweek = matchups[matchups.week <= week]\n",
    "        # weekly sum of pts allowed by a given defense to each position\n",
    "        weekly_sums = matchweek.groupby(['week','defense','position'])[agg_col].sum().reset_index()\n",
    "        # season-to-date mean of weekly sums for each position\n",
    "        defense_pts_allowed = weekly_sums.groupby(['defense','position'])[agg_col].mean().reset_index()\n",
    "        defense_pts_allowed = defense_pts_allowed.rename(columns={agg_col:output_name})\n",
    "        defense_pts_allowed['week'] = week\n",
    "        defense_ranks_dfs.append(defense_pts_allowed)\n",
    "    defense_ranks = pd.concat(defense_ranks_dfs)\n",
    "    return defense_ranks\n",
    "\n",
    "def weekly_player_weights(matchups, weeks):\n",
    "    \n",
    "    \"\"\"Calculate season-to-date (STD) weekly fantasy points rankings by position.\"\"\"\n",
    "    \n",
    "    player_weights = []\n",
    "    for week in weeks:\n",
    "        mask = (matchups.week <= week)\n",
    "        # each player's mean fantasy points STD\n",
    "        std_mean = matchups[mask][['id','team','position','fantasy_points','defense']]\n",
    "        std_mean = std_mean.groupby(['position','id'], as_index=False).mean()\n",
    "\n",
    "        # each player's weight in a given week with respect to their position.\n",
    "        # This is the STD mean max-normalized for the current week.\n",
    "        week_max_position = std_mean.groupby('position', as_index=False).max()\n",
    "        week_max_position = week_max_position[['position','fantasy_points']]\n",
    "        week_max_position.columns = ['position','fp_max']\n",
    "        weekly_weights = std_mean.merge(week_max_position,how='left',on='position')\n",
    "        weekly_weights['player_weight'] = weekly_weights['fantasy_points'] / weekly_weights['fp_max']\n",
    "        weekly_weights['week'] = week\n",
    "        player_weights.append(weekly_weights)\n",
    "    player_weights = pd.concat(player_weights)\n",
    "    return player_weights[['id','week','position','player_weight']]\n",
    "\n",
    "def create_nfl_features(player_stats_orig, opp_stats_orig):\n",
    "    \n",
    "    \"\"\"Wrapper function that calls all helpers to create custom player and team\n",
    "       defense stats. This function will return a new dataframe that has merged\n",
    "       all of the custom stats described in the helper functions for each player.\n",
    "       \n",
    "       Parameters:\n",
    "                   player_stats_orig: game summary actuals for each player weekly\n",
    "                   opp_stats_orig:    matchups for each game (can substitute with\n",
    "                                      a schedule with player_id, offense, defense, week)\"\"\"\n",
    "    \n",
    "    player_stats_trimmed = trim_sort(player_stats_orig)\n",
    "    weeks = sorted(player_stats_trimmed.week.unique().tolist())\n",
    "\n",
    "    # create offensive player stats\n",
    "    trend_df         = get_trend(player_stats_trimmed)\n",
    "    cumavg_stats     = get_cumul_mean_stats(player_stats_trimmed, weeks)\n",
    "    cumavg_stats_wgt = get_cumul_stats_time_weighted(player_stats_trimmed, weeks)\n",
    "\n",
    "    # create matchups and defensive opponent stats\n",
    "    matchup_cols = ['id', 'week', 'team','position', 'full_name', 'offense', 'defense','fantasy_points']\n",
    "    sched             = opp_stats_orig[['offense','defense','week']]\n",
    "    matchups          = player_stats_trimmed.merge(sched, how='left',\n",
    "                                                   left_on=['week','team'],\n",
    "                                                   right_on=['week','offense'])[matchup_cols]\n",
    "    defense_ranks     = defensive_ptsallow(matchups, weeks)\n",
    "    player_weights    = weekly_player_weights(matchups, weeks)\n",
    "    player_weights['inverse'] = 1/player_weights.player_weight\n",
    "    matchups_wgts     = matchups.merge(player_weights, how='left', on=['id','week','position'])\n",
    "    matchups_wgts['weighted_fantasy_points'] = matchups_wgts['fantasy_points'] * matchups_wgts['inverse']\n",
    "    defense_ranks_wgt = defensive_ptsallow(matchups_wgts, weeks, weighted=True)\n",
    "    \n",
    "    ## merge features\n",
    "\n",
    "    # shift target variable, week, and defensive opponent\n",
    "    matchups['target_defense'] = matchups.sort_values(['id','week']).groupby('id')['defense'].shift(-1)\n",
    "    matchups['target'] = matchups.sort_values(['id','week']).groupby('id')['fantasy_points'].shift(-1)\n",
    "    matchups['target_week'] = matchups.sort_values(['id','week']).groupby('id')['week'].shift(-1)\n",
    "\n",
    "    # drop week 1\n",
    "    matchups.dropna(inplace=True)\n",
    "\n",
    "#     # fill in zeros for players with missing historical stats\n",
    "#     matchups.fillna(0, inplace=True)\n",
    "\n",
    "    # merge player weights to player performances\n",
    "    matchups = matchups.merge(player_weights, on=['id','week','position'])\n",
    "\n",
    "    # merge defense rankings to player performances\n",
    "    defense_ranks_all = defense_ranks.merge(defense_ranks_wgt, on=['defense', 'position', 'week'])\n",
    "    defense_ranks_all = defense_ranks_all.rename(columns={'defense':'target_defense'})\n",
    "    matchups = matchups.merge(defense_ranks_all, how='left', on=['target_defense','position','week']).dropna()\n",
    "\n",
    "    # merge trend, average, and weighted avg stats to player performances\n",
    "    avgs = cumavg_stats.merge(cumavg_stats_wgt,how='inner',on=['id','week'])\n",
    "    matchups = matchups.merge(avgs, how='left', on=['id','week'])\n",
    "    trend_cols = ['id','week']+[col for col in trend_df if col not in matchups.columns]\n",
    "    td = trend_df[trend_cols]\n",
    "    matchups = matchups.merge(td, how='inner', on=['id','week'])\n",
    "    for col in trend_df.columns:\n",
    "        matchups[col].fillna(0, inplace=True)\n",
    "\n",
    "    # create extra player attributes and merge to make model-ready df\n",
    "    attribs = ['id','birthdate','years_pro','height','weight','position','profile_url','last_name','number']\n",
    "    player_attributes = player_stats_orig[attribs]\n",
    "    player_attributes.drop_duplicates(['id'],inplace=True)\n",
    "    player_attributes['birthdate'] = pd.to_datetime(player_attributes['birthdate'])\n",
    "    player_attributes['age'] = player_attributes['birthdate'].apply(lambda x: (datetime.today() - x).days/365)\n",
    "    position_dummies = pd.get_dummies(player_attributes['position'])\n",
    "    player_attributes = pd.concat([position_dummies, player_attributes], axis=1).drop(['position'],axis=1)\n",
    "\n",
    "    # final cleaning\n",
    "    model_df = player_attributes.merge(matchups, how='right',on='id')\n",
    "    model_df.replace([-np.inf,np.inf], 0, inplace=True)\n",
    "    return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:06.919987Z",
     "start_time": "2017-12-14T17:01:50.771694Z"
    }
   },
   "outputs": [],
   "source": [
    "nfl_dfs = {year:create_nfl_features(df, opp_dfs[year]) \n",
    "                         for year, df in player_dfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge FanDuel salaries\n",
    "I used FanDuel salaries because it was the only DFS service which has weekly historical salaries that go back to 2013 season. I scraped salaries from the [RotoGuru repo](http://rotoguru1.com/cgi-bin/fyday.pl?game=fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:06.989599Z",
     "start_time": "2017-12-14T17:03:06.921682Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './data/FanDuel/'\n",
    "files = os.listdir(path)\n",
    "fanduel_dfs = {}\n",
    "for fn in files:\n",
    "    yr = int(re.sub('[^0-9]', '', fn))\n",
    "    fanduel_dfs[yr] = pd.read_csv(path+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:07.104049Z",
     "start_time": "2017-12-14T17:03:06.991314Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_salaries(df, year):\n",
    "    positions = ['QB', 'RB', 'WR', 'TE']\n",
    "    df['FirstName'] = df['FirstName'].str.strip()\n",
    "    df['LastName'] = df['LastName'].str.strip()\n",
    "    df['full_name'] = df['LastName']+' '+df['FirstName']\n",
    "    df = df[df.Pos.isin(positions)].fillna(0)[['Week','Team','full_name','fd_points','fd_salary']]\n",
    "    df.columns = ['week','team','full_name','fd_points','fd_salary']\n",
    "    df['week'] = pd.to_numeric(df['week'])\n",
    "    df['fd_points'] = pd.to_numeric(df['fd_points'])\n",
    "    df['fd_salary'] = pd.to_numeric(df['fd_salary'])\n",
    "    df['team'] = df['team'].str.upper()\n",
    "    df['team'] = df['team'].str.replace('KAN','KC')\n",
    "    df['team'] = df['team'].str.replace('GNB','GB')\n",
    "    df['team'] = df['team'].str.replace('LAR','LA')\n",
    "    df['team'] = df['team'].str.replace('STL','LA')\n",
    "    df['team'] = df['team'].str.replace('TAM','TB')\n",
    "    df['team'] = df['team'].str.replace('SFO','SF')\n",
    "    df['team'] = df['team'].str.replace('NOR','NO')\n",
    "    df['team'] = df['team'].str.replace('NWE','NE')\n",
    "    df['team'] = df['team'].str.replace('SDG','SD')\n",
    "    df['team'] = df['team'].str.replace('LAC','SD')\n",
    "    return df\n",
    "\n",
    "def merge_salaries(players, salaries):\n",
    "    new_players = {}\n",
    "    for year in players.keys():\n",
    "        players_year = players[year]\n",
    "        salaries_year = clean_salaries(salaries[year], year)\n",
    "        players_teams = set(players_year.team)\n",
    "        salaries_teams = set(salaries_year.team)\n",
    "        # check that there are no team naming conflicts\n",
    "        print(year, salaries_teams ^ players_teams)\n",
    "        salary_merge = players_year.merge(salaries_year,how='left',on=['week','full_name','team'])\n",
    "        new_players[year] = salary_merge.fillna(0)\n",
    "    return new_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:08.133382Z",
     "start_time": "2017-12-14T17:03:07.112731Z"
    }
   },
   "outputs": [],
   "source": [
    "nfl_fanduel_dfs = merge_salaries(nfl_dfs, fanduel_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:08.153864Z",
     "start_time": "2017-12-14T17:03:08.141057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Year column got lost when I called create_nfl_features() on \n",
    "# each dataframe, so I am adding it back\n",
    "\n",
    "def add_year(df, yr):\n",
    "    df['year'] = yr\n",
    "    return df\n",
    "\n",
    "for year in nfl_fanduel_dfs.keys():\n",
    "    df_year = nfl_fanduel_dfs[year]\n",
    "    nfl_fanduel_dfs[year] = add_year(df_year, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge snap counts\n",
    "Snap counts are the number of times a player is on the field for a play. [Football Outsiders](http://www.footballoutsiders.com/stats/snapcounts) has a historical database of weekly snap counts for each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:08.224089Z",
     "start_time": "2017-12-14T17:03:08.155566Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_snap_counts(dfs):\n",
    "    path = './data/FO_data/SnapCounts/'\n",
    "    SC_files = os.listdir(path)\n",
    "\n",
    "    SC_dfs = []\n",
    "    for fn in SC_files:\n",
    "        week = int(fn[:2])\n",
    "        year = int(fn[3:7])\n",
    "        df = pd.read_csv(path+fn)\n",
    "        df['week'] = week\n",
    "        df['year'] = year\n",
    "        SC_dfs.append(df)\n",
    "    sc = pd.concat(SC_dfs)\n",
    "\n",
    "    sc['number'] = pd.to_numeric(sc.Player.str.replace('[^0-9]',''))\n",
    "    sc['last_name'] = sc.Player.str.replace('[A-Za-z\\s]+\\.','')\n",
    "    sc['last_name'] = sc.last_name.str.replace('[^A-Za-z\\s]','').str.strip()\n",
    "    sc['started'] = sc['Started'].apply(lambda x: 1 if x=='YES' else 0)\n",
    "    sc['offensive_snap_pct'] = pd.to_numeric(sc['Off Snap Pct'].str.replace('[^0-9]',''), \n",
    "                                             downcast='float')\n",
    "    sc['offensive_snap_tot'] = sc['Off Snaps']\n",
    "    sc['position'] = sc.Position.str.replace('FB','RB').str.strip()\n",
    "    sc['team'] = sc['Team'].str.replace('LARM/STL','LA')\n",
    "    sc['team'] = sc['team'].str.replace('LAR','LA')\n",
    "    sc['team'] = sc['team'].str.replace('LAC','SD').str.strip()\n",
    "\n",
    "    sc = sc[sc.position.isin(['QB','RB','WR','TE'])]\n",
    "    sc = sc[['number','last_name','started','offensive_snap_pct',\n",
    "             'offensive_snap_tot','position','team','week','year']]\n",
    "    new_dfs = {}\n",
    "    for year, df in dfs.items():\n",
    "        merge_df = df.merge(sc, on=['last_name','team','week','year','position'])\n",
    "        new_dfs[year] = merge_df\n",
    "    return new_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:11.426499Z",
     "start_time": "2017-12-14T17:03:08.226494Z"
    }
   },
   "outputs": [],
   "source": [
    "nfl_fd_sc_dfs = merge_snap_counts(nfl_fanduel_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge gameday weather forecasts\n",
    "Gameday weather forecasts were scraped from [http://nflweather.com/](http://nflweather.com/). I only care about wind speeds as this is factor that can impact the offense's decision to throw the ball versus running the ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:11.493291Z",
     "start_time": "2017-12-14T17:03:11.428076Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_weather(dfs):\n",
    "    from data.nfl_teams import team_dict\n",
    "    path = './data/NflWeather/'\n",
    "    weather_files = os.listdir(path)\n",
    "    weather_dfs = []\n",
    "    for fn in weather_files:\n",
    "        week = re.findall('_[0-9]+\\.',fn)\n",
    "        week = re.sub('[^0-9]','',str(week))\n",
    "        year = fn[:4]\n",
    "        df = pd.read_csv(path+fn)\n",
    "        df['week'] = int(week)\n",
    "        df['year'] = int(year)\n",
    "        weather_dfs.append(df)\n",
    "    weather = pd.concat(weather_dfs)\n",
    "\n",
    "    weather['team1'] = weather['team1'].apply(lambda x: team_dict[x])\n",
    "    weather['team2'] = weather['team2'].apply(lambda x: team_dict[x])\n",
    "\n",
    "    weather['wind_conditions'] = pd.to_numeric(weather['wind_conditions'].str.replace('[^0-9]',''))\n",
    "    weather['indoor_outdoor'] = weather['weather_forecast'].apply(lambda x: 1 if 'DOME' in x else 0)\n",
    "\n",
    "    weather1 = weather[['team1','wind_conditions','indoor_outdoor','week','year']]\n",
    "    weather1.columns = ['team','wind_conditions','indoor_outdoor','week','year']\n",
    "    weather2 = weather[['team2','wind_conditions','indoor_outdoor','week','year']]\n",
    "    weather2.columns = ['team','wind_conditions','indoor_outdoor','week','year']\n",
    "    weather = pd.concat([weather1,weather2])\n",
    "    \n",
    "    new_dfs = {}\n",
    "    for year, df in dfs.items():\n",
    "        merge_df = df.merge(weather,on=['team','week','year'])\n",
    "        new_dfs[year] = merge_df\n",
    "    return new_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:11.866532Z",
     "start_time": "2017-12-14T17:03:11.496380Z"
    }
   },
   "outputs": [],
   "source": [
    "nfl_fd_sc_weather_dfs = merge_weather(nfl_fd_sc_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and test sets\n",
    "Now that all features have been cleaned and merged to a dataframe for each season, I will merge historical seasons to be used for training/validating my regression model. I will use the current season (2017) as a holdout test set.\n",
    "- \"model_df\" = train/validation set (2013-2016 seasons)\n",
    "- \"holdout\" = test set (2017 / current season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:11.873612Z",
     "start_time": "2017-12-14T17:03:11.868107Z"
    }
   },
   "outputs": [],
   "source": [
    "for year in range(2013,2018):\n",
    "    print(nfl_fd_sc_weather_dfs[year].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:11.972363Z",
     "start_time": "2017-12-14T17:03:11.878525Z"
    }
   },
   "outputs": [],
   "source": [
    "max_year = max(list(nfl_fd_sc_weather_dfs.keys()))\n",
    "holdout = nfl_fd_sc_weather_dfs[2017]\n",
    "model_dfs = [nfl_fd_sc_weather_dfs[yr] for yr in nfl_fd_sc_weather_dfs.keys() if yr != max_year]\n",
    "model_df = pd.concat(model_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:12.033763Z",
     "start_time": "2017-12-14T17:03:11.973884Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:12.054207Z",
     "start_time": "2017-12-14T17:03:12.036943Z"
    }
   },
   "outputs": [],
   "source": [
    "holdout.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:12.063000Z",
     "start_time": "2017-12-14T17:03:12.058106Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:12.074998Z",
     "start_time": "2017-12-14T17:03:12.064533Z"
    }
   },
   "outputs": [],
   "source": [
    "holdout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert target to target rank\n",
    "I experimented with changing the target variable to a weekly rank instead of just using the fantasy points for that week, but I did not end up using this in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:12.187054Z",
     "start_time": "2017-12-14T17:03:12.077368Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df = model_df.sort_values(['year','position','week','target'], ascending=False).reset_index(drop=True)\n",
    "model_df['target_rank'] = model_df.groupby(['year','position','week'])['target'].\\\n",
    "                                    rank(method=\"dense\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:12.222302Z",
     "start_time": "2017-12-14T17:03:12.189575Z"
    }
   },
   "outputs": [],
   "source": [
    "holdout.sort_values(['year','position','week','target'], ascending=False, inplace=True)\n",
    "holdout['target_rank'] = holdout.groupby(['year','position','week'])['target'].\\\n",
    "                                    rank(method=\"dense\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:12.532396Z",
     "start_time": "2017-12-14T17:03:12.224095Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df['target_rank'].hist(bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge ESPN benchmarks\n",
    "I scraped fantasy point projections from [ESPN](http://games.espn.com/ffl/tools/projections?) for each week in the current season and merge to holdout test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:13.077770Z",
     "start_time": "2017-12-14T17:03:12.534382Z"
    }
   },
   "outputs": [],
   "source": [
    "proj = pd.read_json(\"./data/espn_projections.json\")\n",
    "\n",
    "clean_team_pos = proj['team'].str.replace('[^a-zA-Z\\s]', '').str.upper().str.split(expand=True)\n",
    "proj.drop(['team','position'],axis=1,inplace=True)\n",
    "clean_team_pos.columns= ['team', 'position']\n",
    "\n",
    "proj = pd.concat([proj, clean_team_pos], axis=1)\n",
    "\n",
    "proj['team'] = proj['team'].str.replace('LAR','LA')\n",
    "proj['team'] = proj['team'].str.replace('WSH','WAS')\n",
    "proj['team'] = proj['team'].str.replace('LAC','SD')\n",
    "proj['team'] = proj['team'].str.replace('JAX','JAC')\n",
    "\n",
    "proj = proj[['name','position','team','week','proj_pts']]\n",
    "\n",
    "proj.columns = ['full_name','position','team','target_week','proj_pts']\n",
    "proj = proj[proj.proj_pts != '--']\n",
    "proj['proj_pts'] = pd.to_numeric(proj['proj_pts'])\n",
    "proj = proj[~proj.proj_pts.isnull()]\n",
    "proj['full_name'] = proj.full_name.str.replace('Jr.','').str.strip()\n",
    "proj['full_name'] = proj.full_name.str.replace('Sr.','').str.strip()\n",
    "proj['full_name'] = proj.full_name.str.replace('III','').str.strip()\n",
    "proj['full_name'] = proj.full_name.str.replace('II','').str.strip()\n",
    "convert_names = {'Joshua Bellamy':'Josh Bellamy',\n",
    "                 'TJ Jones':'T.J. Jones',\n",
    "                 'Will Fuller V':'Will Fuller',\n",
    "                 'Matthew Dayes':'Matt Dayes'}\n",
    "proj['full_name'] = proj.full_name.replace(convert_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:13.168013Z",
     "start_time": "2017-12-14T17:03:13.079451Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge ESPN projections to holdout set\n",
    "merge_cols = ['full_name','position','team','target_week']\n",
    "holdout = holdout.merge(proj, how='left', on=merge_cols)\n",
    "holdout.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:13.198245Z",
     "start_time": "2017-12-14T17:03:13.169818Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a rank for ESPN projections\n",
    "holdout.sort_values(['year','position','week','proj_pts'], ascending=False, inplace=True)\n",
    "holdout['espn_rank'] = holdout.groupby(['year','position','week'])['proj_pts'].\\\n",
    "                                    rank(method=\"dense\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:13.206606Z",
     "start_time": "2017-12-14T17:03:13.199975Z"
    }
   },
   "outputs": [],
   "source": [
    "holdout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trim outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:13.515959Z",
     "start_time": "2017-12-14T17:03:13.213057Z"
    }
   },
   "outputs": [],
   "source": [
    "# trim players who scored zero in a given week because they \n",
    "# are skewing the distribution for my target variable.\n",
    "\n",
    "thresh = 0\n",
    "\n",
    "tmp = model_df[model_df.target > thresh]\n",
    "print(model_df.shape)\n",
    "print(tmp.shape)\n",
    "tmp.target.hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:13.834994Z",
     "start_time": "2017-12-14T17:03:13.518189Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = holdout[holdout.target > thresh]\n",
    "print(holdout.shape)\n",
    "print(tmp.shape)\n",
    "tmp.target.hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.137452Z",
     "start_time": "2017-12-14T17:03:13.836686Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = holdout[holdout.proj_pts > thresh]\n",
    "print(holdout.shape)\n",
    "print(tmp.shape)\n",
    "tmp.proj_pts.hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.202071Z",
     "start_time": "2017-12-14T17:03:14.139492Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df_trimmed = model_df[model_df.target > thresh].reset_index(drop=True)\n",
    "holdout_trimmed = holdout[(holdout.target > thresh) & \n",
    "                          (holdout.proj_pts > thresh)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT RUN: boxcox transform\n",
    "The target distribution is pretty skewed, so I use a boxcox transform to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.215505Z",
     "start_time": "2017-12-14T17:03:14.211233Z"
    }
   },
   "outputs": [],
   "source": [
    "# from scipy.stats import boxcox\n",
    "\n",
    "# model_df_trimmed['target'] = pd.Series(boxcox(model_df_trimmed.target)[0])\n",
    "# holdout_trimmed['target'] = pd.Series(boxcox(holdout_trimmed.target)[0])\n",
    "# holdout_trimmed['proj_pts'] = pd.Series(boxcox(holdout_trimmed.proj_pts)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.226336Z",
     "start_time": "2017-12-14T17:03:14.217803Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_df_trimmed.target.hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.231063Z",
     "start_time": "2017-12-14T17:03:14.227965Z"
    }
   },
   "outputs": [],
   "source": [
    "# holdout_trimmed.target.hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.242735Z",
     "start_time": "2017-12-14T17:03:14.233142Z"
    }
   },
   "outputs": [],
   "source": [
    "# holdout_trimmed.proj_pts.hist(bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## player stat interactions\n",
    "The following merges serve the purpose to inform a given player of what type of offense their team employs. For instance, merging the mean QB pass/rush attempts ratio to that QB's WR gives an idea of how often the QB runs versus throwing, thus affecting the possible targets thrown to that WR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.300338Z",
     "start_time": "2017-12-14T17:03:14.244977Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_df_columns = model_df_trimmed.select_dtypes(include=['float32','int32','int64','float64','uint8']).columns\n",
    "fumble_features = [c for c in numeric_df_columns if \"fumble\" in c and \"per\" not in c]\n",
    "\n",
    "passing_features = ['passing_att_mean','passing_cmp_mean','passing_ints_mean','passing_yds_mean',\n",
    "                    'passing_tds_mean','passer_ratio','PassRushRatio_Att_mean','PassRushRatio_Yds_mean',\n",
    "                    'PassRushRatio_Tds_mean']\n",
    "\n",
    "rushing_features = ['rushing_yds_mean','rushing_att_mean','rushing_tds_mean',\n",
    "                    'RushRecRatio_AttRec_mean','RushRecRatio_Tds_mean','RushRecRatio_Yds_mean']\n",
    "\n",
    "receiving_features = ['receiving_rec_mean','receiving_tds_mean','receiving_yds_mean']\n",
    "\n",
    "sharedfeats = ['full_name','team','years_pro','age','weight','height','player_weight','year','week','target',\n",
    "               'target_rank','fantasy_points_mean','target_week','defensive_matchup_allowed','offensive_snap_pct',\n",
    "               'offensive_snap_tot','fd_salary','started','wind_conditions','indoor_outdoor']+fumble_features\n",
    "\n",
    "QB_features = sharedfeats+passing_features+rushing_features\n",
    "RB_features = sharedfeats+rushing_features+receiving_features\n",
    "WR_features = sharedfeats+receiving_features\n",
    "TE_features = sharedfeats+receiving_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.432700Z",
     "start_time": "2017-12-14T17:03:14.305629Z"
    }
   },
   "outputs": [],
   "source": [
    "QB_df = model_df_trimmed[model_df_trimmed.position == 'QB'][QB_features].reset_index().drop('index',axis=1)\n",
    "QB_stats = QB_df.groupby(['year','week','team']).mean()[passing_features].reset_index().add_prefix('QBMEAN')\n",
    "QB_stats=QB_stats.rename(columns={'QBMEANyear':'year', 'QBMEANweek':'week', 'QBMEANteam':'team'})\n",
    "QB_merge_stats = ['QBMEANPassRushRatio_Att','QBMEANPassRushRatio_Yds','QBMEANPassRushRatio_Tds']\n",
    "\n",
    "RB_df = model_df_trimmed[model_df_trimmed.position == 'RB'][RB_features].reset_index().drop('index',axis=1)\n",
    "RB_stats = RB_df.groupby(['year','week','team']).mean()[rushing_features].reset_index().add_prefix('RBMEAN')\n",
    "RB_stats=RB_stats.rename(columns={'RBMEANyear':'year', 'RBMEANweek':'week', 'RBMEANteam':'team'})\n",
    "\n",
    "WR_df = model_df_trimmed[model_df_trimmed.position == 'WR'][WR_features].reset_index().drop('index',axis=1)\n",
    "WR_stats = WR_df.groupby(['year','week','team']).mean()[receiving_features].reset_index().add_prefix('WRMEAN')\n",
    "WR_stats=WR_stats.rename(columns={'WRMEANyear':'year', 'WRMEANweek':'week', 'WRMEANteam':'team'})\n",
    "\n",
    "TE_df = model_df_trimmed[model_df_trimmed.position == 'TE'][TE_features].reset_index().drop('index',axis=1)\n",
    "TE_stats = TE_df.groupby(['year','week','team']).mean()[receiving_features].reset_index().add_prefix('TEMEAN')\n",
    "TE_stats=TE_stats.rename(columns={'TEMEANyear':'year', 'TEMEANweek':'week', 'TEMEANteam':'team'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.443754Z",
     "start_time": "2017-12-14T17:03:14.435141Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"QB:\", QB_df.shape)\n",
    "print(\"RB:\", RB_df.shape)\n",
    "print(\"WR:\", WR_df.shape)\n",
    "print(\"TE:\", TE_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.501130Z",
     "start_time": "2017-12-14T17:03:14.445588Z"
    }
   },
   "outputs": [],
   "source": [
    "QB_df = QB_df.merge(RB_stats, how='inner', on=['year','week','team'])\n",
    "\n",
    "RB_df = RB_df.merge(QB_stats, how='inner', on=['year','week','team'])\n",
    "\n",
    "WR_df = WR_df.merge(QB_stats, how='inner', on=['year','week','team'])\n",
    "WR_df = WR_df.merge(RB_stats, how='inner', on=['year','week','team'])\n",
    "\n",
    "TE_df = TE_df.merge(QB_stats, how='inner', on=['year','week','team'])\n",
    "TE_df = TE_df.merge(RB_stats, how='inner', on=['year','week','team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.510737Z",
     "start_time": "2017-12-14T17:03:14.504597Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"QB:\", QB_df.shape)\n",
    "print(\"RB:\", RB_df.shape)\n",
    "print(\"WR:\", WR_df.shape)\n",
    "print(\"TE:\", TE_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.523732Z",
     "start_time": "2017-12-14T17:03:14.513513Z"
    }
   },
   "outputs": [],
   "source": [
    "QB_features = QB_features+['espn_rank','proj_pts']\n",
    "RB_features = RB_features+['espn_rank','proj_pts']\n",
    "WR_features = WR_features+['espn_rank','proj_pts']\n",
    "TE_features = TE_features+['espn_rank','proj_pts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.647687Z",
     "start_time": "2017-12-14T17:03:14.525313Z"
    }
   },
   "outputs": [],
   "source": [
    "QB_df_holdout = holdout_trimmed[holdout_trimmed.position == 'QB'][QB_features].reset_index().drop('index',axis=1)\n",
    "QB_stats = QB_df_holdout.groupby(['year','week','team']).mean()[passing_features].reset_index().add_prefix('QBMEAN')\n",
    "QB_stats=QB_stats.rename(columns={'QBMEANyear':'year', 'QBMEANweek':'week', 'QBMEANteam':'team'})\n",
    "QB_merge_stats = ['QBMEANPassRushRatio_Att','QBMEANPassRushRatio_Yds','QBMEANPassRushRatio_Tds']\n",
    "\n",
    "RB_df_holdout = holdout_trimmed[holdout_trimmed.position == 'RB'][RB_features].reset_index().drop('index',axis=1)\n",
    "RB_stats = RB_df_holdout.groupby(['year','week','team']).mean()[rushing_features].reset_index().add_prefix('RBMEAN')\n",
    "RB_stats=RB_stats.rename(columns={'RBMEANyear':'year', 'RBMEANweek':'week', 'RBMEANteam':'team'})\n",
    "\n",
    "WR_df_holdout = holdout_trimmed[holdout_trimmed.position == 'WR'][WR_features].reset_index().drop('index',axis=1)\n",
    "WR_stats = WR_df_holdout.groupby(['year','week','team']).mean()[receiving_features].reset_index().add_prefix('WRMEAN')\n",
    "WR_stats=WR_stats.rename(columns={'WRMEANyear':'year', 'WRMEANweek':'week', 'WRMEANteam':'team'})\n",
    "\n",
    "TE_df_holdout = holdout_trimmed[holdout_trimmed.position == 'TE'][TE_features].reset_index().drop('index',axis=1)\n",
    "TE_stats = TE_df_holdout.groupby(['year','week','team']).mean()[receiving_features].reset_index().add_prefix('TEMEAN')\n",
    "TE_stats=TE_stats.rename(columns={'TEMEANyear':'year', 'TEMEANweek':'week', 'TEMEANteam':'team'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.686584Z",
     "start_time": "2017-12-14T17:03:14.659493Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"QB:\", QB_df_holdout.shape)\n",
    "print(\"RB:\", RB_df_holdout.shape)\n",
    "print(\"WR:\", WR_df_holdout.shape)\n",
    "print(\"TE:\", TE_df_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.840322Z",
     "start_time": "2017-12-14T17:03:14.690407Z"
    }
   },
   "outputs": [],
   "source": [
    "QB_df_holdout = QB_df_holdout.merge(RB_stats, how='inner', on=['year','week','team'])\n",
    "\n",
    "RB_df_holdout = RB_df_holdout.merge(QB_stats, how='inner', on=['year','week','team'])\n",
    "\n",
    "WR_df_holdout = WR_df_holdout.merge(RB_stats, how='inner', on=['year','week','team'])\n",
    "WR_df_holdout = WR_df_holdout.merge(QB_stats, how='inner', on=['year','week','team'])\n",
    "\n",
    "TE_df_holdout = TE_df_holdout.merge(QB_stats, how='inner', on=['year','week','team'])\n",
    "TE_df_holdout = TE_df_holdout.merge(RB_stats, how='inner', on=['year','week','team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:03:14.875043Z",
     "start_time": "2017-12-14T17:03:14.847924Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"QB:\", QB_df_holdout.shape)\n",
    "print(\"RB:\", RB_df_holdout.shape)\n",
    "print(\"WR:\", WR_df_holdout.shape)\n",
    "print(\"TE:\", TE_df_holdout.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## interactions by position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:05:10.061421Z",
     "start_time": "2017-12-14T17:03:14.883822Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "create_interactions = [('QB',QB_df), ('RB',RB_df), ('WR',WR_df), ('TE',TE_df)]\n",
    "interactions_positions = {}\n",
    "for pos, data in create_interactions:\n",
    "    numerics = data.select_dtypes(include=['float32','int32','int64','float64','uint8']).columns.tolist()\n",
    "    data = data[numerics]\n",
    "    poly = PolynomialFeatures(2, interaction_only=True, include_bias=True)\n",
    "    interactions_arr = poly.fit_transform(data)\n",
    "    interactions_names = poly.get_feature_names(data.columns)\n",
    "    interactions = pd.DataFrame(interactions_arr, columns=interactions_names)\n",
    "    interactions_positions[pos] = interactions\n",
    "\n",
    "def explore_interaction_corrs(pos, interactions):\n",
    "    target_col = interactions['target']\n",
    "    nontarget_interactions = interactions[[c for c in interactions.columns if \"target\" not in c]]\n",
    "    explore = pd.concat([target_col, nontarget_interactions], axis=1)\n",
    "    corr_interactions = explore.corr()\n",
    "    return corr_interactions[['target']].sort_values('target',ascending=False).reset_index()\n",
    "\n",
    "QB_inters = explore_interaction_corrs('QB', interactions_positions['QB'])\n",
    "RB_inters = explore_interaction_corrs('RB', interactions_positions['RB'])\n",
    "WR_inters = explore_interaction_corrs('WR', interactions_positions['WR'])\n",
    "TE_inters = explore_interaction_corrs('TE', interactions_positions['TE'])\n",
    "\n",
    "# QB_inters.to_csv('data/interactions_QB.csv')\n",
    "# RB_inters.to_csv('data/interactions_RB.csv')\n",
    "# WR_inters.to_csv('data/interactions_WR.csv')\n",
    "# TE_inters.to_csv('data/interactions_TE.csv')\n",
    "\n",
    "# QB_inters = pd.read_csv('data/interactions_QB.csv')\n",
    "# RB_inters = pd.read_csv('data/interactions_RB.csv')\n",
    "# WR_inters = pd.read_csv('data/interactions_WR.csv')\n",
    "# TE_inters = pd.read_csv('data/interactions_TE.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:05:10.095236Z",
     "start_time": "2017-12-14T17:05:10.066113Z"
    }
   },
   "outputs": [],
   "source": [
    "QB_inters.sort_values('target',ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### RB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:05:10.119620Z",
     "start_time": "2017-12-14T17:05:10.102970Z"
    }
   },
   "outputs": [],
   "source": [
    "RB_inters.sort_values('target',ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:05:10.140761Z",
     "start_time": "2017-12-14T17:05:10.121366Z"
    }
   },
   "outputs": [],
   "source": [
    "WR_inters.sort_values('target',ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:05:10.179104Z",
     "start_time": "2017-12-14T17:05:10.147715Z"
    }
   },
   "outputs": [],
   "source": [
    "TE_inters.sort_values('target',ascending=False).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:05:10.233051Z",
     "start_time": "2017-12-14T17:05:10.182753Z"
    }
   },
   "outputs": [],
   "source": [
    "QB_df.reset_index(inplace=True,drop=True)\n",
    "RB_df.reset_index(inplace=True,drop=True)\n",
    "WR_df.reset_index(inplace=True,drop=True)\n",
    "TE_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "QB_df['chg_fumbles_yds trend_fumbles_tot'] = QB_df['trend_fumbles_tot']*QB_df['chg_fumbles_yds']\n",
    "QB_df['age player_weight'] = QB_df['age']*QB_df['player_weight']\n",
    "RB_df['player_weight offensive_snap_pct'] = RB_df['player_weight']*RB_df['offensive_snap_pct']\n",
    "RB_df['fantasy_points_mean offensive_snap_tot'] = RB_df['offensive_snap_tot']*RB_df['fantasy_points_mean']\n",
    "RB_df['offensive_snap_tot chg_fumbles_tot'] = RB_df['offensive_snap_tot']*RB_df['chg_fumbles_tot']\n",
    "WR_df['chg_fumbles_tot receiving_yds_mean'] = WR_df['chg_fumbles_tot']*WR_df['receiving_yds_mean']\n",
    "WR_df['age receiving_yds_mean'] = WR_df['age']*WR_df['receiving_yds_mean']\n",
    "TE_df['trend_fumbles_tot RBMEANRushRecRatio_AttRec_mean'] = TE_df['trend_fumbles_tot']*TE_df['RBMEANRushRecRatio_AttRec_mean']\n",
    "TE_df['age receiving_yds_mean'] = TE_df['age']*TE_df['receiving_yds_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:05:10.308934Z",
     "start_time": "2017-12-14T17:05:10.245134Z"
    }
   },
   "outputs": [],
   "source": [
    "QB_df_holdout.reset_index(inplace=True,drop=True)\n",
    "RB_df_holdout.reset_index(inplace=True,drop=True)\n",
    "WR_df_holdout.reset_index(inplace=True,drop=True)\n",
    "TE_df_holdout.reset_index(inplace=True,drop=True)\n",
    "\n",
    "QB_df_holdout['chg_fumbles_yds trend_fumbles_tot'] = QB_df_holdout['trend_fumbles_tot']*QB_df_holdout['chg_fumbles_yds']\n",
    "QB_df_holdout['age player_weight'] = QB_df_holdout['age']*QB_df_holdout['player_weight']\n",
    "RB_df_holdout['player_weight offensive_snap_pct'] = RB_df_holdout['player_weight']*RB_df_holdout['offensive_snap_pct']\n",
    "RB_df_holdout['fantasy_points_mean offensive_snap_tot'] = RB_df_holdout['offensive_snap_tot']*RB_df_holdout['fantasy_points_mean']\n",
    "RB_df_holdout['offensive_snap_tot chg_fumbles_tot'] = RB_df_holdout['offensive_snap_tot']*RB_df_holdout['chg_fumbles_tot']\n",
    "WR_df_holdout['chg_fumbles_tot receiving_yds_mean'] = WR_df_holdout['chg_fumbles_tot']*WR_df_holdout['receiving_yds_mean']\n",
    "WR_df_holdout['age receiving_yds_mean'] = WR_df_holdout['age']*WR_df_holdout['receiving_yds_mean']\n",
    "TE_df_holdout['trend_fumbles_tot RBMEANRushRecRatio_AttRec_mean'] = TE_df_holdout['trend_fumbles_tot']*TE_df_holdout['RBMEANRushRecRatio_AttRec_mean']\n",
    "TE_df_holdout['age receiving_yds_mean'] = TE_df_holdout['age']*TE_df_holdout['receiving_yds_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory mgmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:05:10.447304Z",
     "start_time": "2017-12-14T17:05:10.317642Z"
    }
   },
   "outputs": [],
   "source": [
    "del player_dfs\n",
    "del opp_dfs\n",
    "del nfl_dfs\n",
    "del fanduel_dfs\n",
    "del nfl_fanduel_dfs\n",
    "del nfl_fd_sc_dfs\n",
    "del nfl_fd_sc_weather_dfs\n",
    "del QB_inters\n",
    "del RB_inters\n",
    "del WR_inters\n",
    "del TE_inters\n",
    "del interactions_positions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:05:10.515394Z",
     "start_time": "2017-12-14T17:05:10.449218Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import sklearn.linear_model as lin\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pprint import pprint\n",
    "import matplotlib\n",
    "from math import exp\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:07:53.040151Z",
     "start_time": "2017-12-14T17:07:53.037518Z"
    }
   },
   "outputs": [],
   "source": [
    "RESPONSE_VAR = 'target'\n",
    "BENCHMARK = 'proj_pts'\n",
    "CURR_WEEK = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:22.448580Z",
     "start_time": "2017-12-14T17:12:22.170581Z"
    }
   },
   "outputs": [],
   "source": [
    "def regression_tts(input_df, week, response):\n",
    "    \n",
    "    \"\"\"Run a regression for each position (QB,RB,WR,TE),\n",
    "       Train on first 14 weeks of each season 2013-2016,\n",
    "       Test on last 3 weeks of each season 2013-2016\"\"\"\n",
    "    \n",
    "    features = input_df.select_dtypes(include=['float32','int32','int64','float64','uint8']).columns.tolist()\n",
    "    features.remove('year')\n",
    "    features.remove('target')\n",
    "    features.remove('target_rank')\n",
    "    features.remove('target_week')\n",
    "    \n",
    "    est = GradientBoostingRegressor(n_estimators=30, learning_rate=0.1)\n",
    "#     est = lin.LassoCV()\n",
    "    \n",
    "    df_train = input_df[input_df.target_week <= week]\n",
    "#     df_train.sort_values('fantasy_points_mean',ascending=True,inplace=True)\n",
    "    df_test = input_df[input_df.target_week > week]\n",
    "    \n",
    "    X_train = df_train[features]\n",
    "    y_train = df_train[response]\n",
    "    X_test = df_test[features]\n",
    "    y_test = df_test[response]\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_test = ss.fit_transform(X_test)\n",
    "    \n",
    "#     sw = np.linspace(.01,1,X_train.shape[0])\n",
    "    est.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = est.predict(X_train)\n",
    "    train_results = (metrics.mean_squared_error(y_train, y_pred))**(0.5)\n",
    "#     train_results = metrics.r2_score(y_train, y_pred)\n",
    "\n",
    "    y_pred = est.predict(X_test)\n",
    "    test_results = (metrics.mean_squared_error(y_test, y_pred))**(0.5)\n",
    "#     test_results = metrics.r2_score(y_test, y_pred)\n",
    "    \n",
    "    df_test = df_test.reset_index()[['target_week','fantasy_points_mean',response]]\n",
    "    df_test['MODEL_PRED'] = pd.Series(y_pred)\n",
    "    df_test['RESIDUALS'] = df_test['MODEL_PRED'] - df_test[response]\n",
    "    \n",
    "    coefs = None\n",
    "    if 'sklearn.linear_model' in est.__module__:\n",
    "        coef_ranks = list(zip(abs(est.coef_), est.coef_, features))\n",
    "        coefs = sorted(coef_ranks, key=lambda x: x[0], reverse=True)\n",
    "    else:\n",
    "        coef_ranks = list(zip(est.feature_importances_, features))\n",
    "        coefs = sorted(coef_ranks, key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "    return test_results, train_results, est, coefs, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:25.049248Z",
     "start_time": "2017-12-14T17:12:22.474564Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coefs_all = {}\n",
    "ests_positions = {}\n",
    "resids_positions = []\n",
    "\n",
    "for pos, data in [('QB',QB_df),('RB',RB_df),('WR',WR_df),('TE',TE_df)]:\n",
    "    print(pos)\n",
    "    results_test, results_train, est, coefs, resids = regression_tts(data, CURR_WEEK, RESPONSE_VAR)\n",
    "    \n",
    "    print('train results:',results_train)\n",
    "    print('test  results:',results_test)\n",
    "    coefs_all[pos] = coefs\n",
    "    ests_positions[pos] = est\n",
    "    resids['position'] = pos\n",
    "    resids_positions.append(resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:28.638476Z",
     "start_time": "2017-12-14T17:12:28.626577Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for pos in coefs_all.keys():\n",
    "    print(pos)\n",
    "    pprint(coefs_all[pos][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:31.183975Z",
     "start_time": "2017-12-14T17:12:31.170066Z"
    }
   },
   "outputs": [],
   "source": [
    "QB_coefs_plot = [x[0] for x in coefs_all['QB']][:3]\n",
    "RB_coefs_plot = [x[0] for x in coefs_all['RB']][:3]\n",
    "WR_coefs_plot = [x[0] for x in coefs_all['WR']][:3]\n",
    "TE_coefs_plot = [x[0] for x in coefs_all['TE']][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:32.486560Z",
     "start_time": "2017-12-14T17:12:31.389467Z"
    }
   },
   "outputs": [],
   "source": [
    "font = {'size'   : 18}\n",
    "matplotlib.rc('font', **font)\n",
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4,figsize=(23,5), sharey=True)\n",
    "\n",
    "for ax, title, pos_coefs in [(ax1,'QB',QB_coefs_plot), (ax2,'RB',RB_coefs_plot),\n",
    "                      (ax3,'WR',WR_coefs_plot), (ax4,'TE',TE_coefs_plot)]:\n",
    "    ax.bar(range(len(WR_coefs_plot)),WR_coefs_plot)\n",
    "    ax.set_title(title)\n",
    "    ax.axes.xaxis.set_ticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:10:19.622214Z",
     "start_time": "2017-12-14T17:10:18.378367Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "font = {'size'   : 16}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "features = RB_df.select_dtypes(include=['float32','int32','int64','float64','uint8']).columns.tolist()\n",
    "features.remove('year')\n",
    "features.remove('target')\n",
    "features.remove('target_rank')\n",
    "features.remove('target_week')\n",
    "\n",
    "fp_mean = features.index('fantasy_points_mean')\n",
    "fd_sal = features.index('fd_salary')\n",
    "pr = features.index('receiving_rec_mean')\n",
    "pw = features.index('player_weight')\n",
    "dpa = features.index('defensive_matchup_allowed')\n",
    "snaps = features.index('offensive_snap_tot')\n",
    "X_train = RB_df[features]\n",
    "X_train = X_train[X_train.week < CURR_WEEK].as_matrix()\n",
    "\n",
    "features_idx = [fp_mean, fd_sal, dpa, pr, snaps, pw]\n",
    "fig, axs = plot_partial_dependence(ests_positions['RB'], X_train, features_idx,\n",
    "                                   feature_names=features,\n",
    "                                   n_jobs=-1, grid_resolution=50, figsize=(20,10))\n",
    "fig.suptitle('Partial dependence for RBs')\n",
    "plt.subplots_adjust(top=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:10:20.680672Z",
     "start_time": "2017-12-14T17:10:20.656931Z"
    }
   },
   "outputs": [],
   "source": [
    "model_results = pd.concat(resids_positions)\n",
    "model_results = model_results.sort_values('MODEL_PRED').reset_index()\n",
    "model_results_QB = resids_positions[0].sort_values('MODEL_PRED').reset_index()\n",
    "model_results_RB = resids_positions[1].sort_values('MODEL_PRED').reset_index()\n",
    "model_results_WR = resids_positions[2].sort_values('MODEL_PRED').reset_index()\n",
    "model_results_TE = resids_positions[3].sort_values('MODEL_PRED').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:02.331140Z",
     "start_time": "2017-12-14T17:12:01.841798Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(model_results.MODEL_PRED, model_results.RESIDUALS, s=3)\n",
    "plt.ylabel('Residual')\n",
    "plt.xlabel('Predicted Fantasy Points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:10:22.476341Z",
     "start_time": "2017-12-14T17:10:22.178469Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(model_results_QB.MODEL_PRED, model_results_QB.RESIDUALS, s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:10:23.175596Z",
     "start_time": "2017-12-14T17:10:22.869041Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(model_results_RB.MODEL_PRED, model_results_RB.RESIDUALS, s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:10:23.603035Z",
     "start_time": "2017-12-14T17:10:23.234813Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(model_results_WR.MODEL_PRED, model_results_WR.RESIDUALS, s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:10:23.982289Z",
     "start_time": "2017-12-14T17:10:23.604897Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(model_results_TE.MODEL_PRED, model_results_TE.RESIDUALS, s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:10:24.401711Z",
     "start_time": "2017-12-14T17:10:23.983779Z"
    }
   },
   "outputs": [],
   "source": [
    "font = {'size'   : 20}\n",
    "matplotlib.rc('font', **font)\n",
    "plt.figure(figsize=(10,6))\n",
    "model_results.RESIDUALS.hist(bins=50)\n",
    "plt.xlabel(\"Residual Value\")\n",
    "plt.ylabel(\"Number of Observations\")\n",
    "print(model_results.RESIDUALS.mean())\n",
    "print(model_results.RESIDUALS.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## holdout test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:07.690442Z",
     "start_time": "2017-12-14T17:12:07.684474Z"
    }
   },
   "outputs": [],
   "source": [
    "# weeks = sorted(model_df.target_week.unique().tolist())\n",
    "# weeks.remove(1.0)\n",
    "# weeks.remove(2.0)\n",
    "weeks_predict = list(range(2,CURR_WEEK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:41.214827Z",
     "start_time": "2017-12-14T17:12:41.100270Z"
    }
   },
   "outputs": [],
   "source": [
    "def weekly_regression_predict(input_df, weeks, est, response, espn_response):\n",
    "\n",
    "    features = input_df.select_dtypes(include=['float32','int32','int64','float64','uint8']).columns.tolist()\n",
    "    features.remove('year')\n",
    "    features.remove('target')\n",
    "    features.remove('target_rank')\n",
    "    features.remove('proj_pts')\n",
    "    features.remove('espn_rank')\n",
    "    \n",
    "    week_nums = []\n",
    "    scores = []\n",
    "    bmk_scores = []\n",
    "    predictions = []\n",
    "\n",
    "    for week in weeks:\n",
    "        week_nums.append(week)\n",
    "       \n",
    "        df_cv = input_df[input_df.target_week == week]\n",
    "        X = df_cv[features]\n",
    "        y = df_cv[response]\n",
    "        y_benchmark = df_cv[espn_response]\n",
    "#         X.sort_values('fantasy_points_mean',ascending=True,inplace=True)\n",
    "        ss = StandardScaler()\n",
    "        X = ss.fit_transform(X)\n",
    "        \n",
    "        y_pred = est.predict(X)\n",
    "\n",
    "#         score = metrics.r2_score(y, y_pred)\n",
    "#         score = (metrics.mean_squared_error(y, y_pred))**(0.5)\n",
    "        score = metrics.mean_absolute_error(y, y_pred)\n",
    "        scores.append(score)\n",
    "    \n",
    "#         bmk_score = metrics.r2_score(y, y_benchmark)\n",
    "#         bmk_score = (metrics.mean_squared_error(y, y_benchmark))**(0.5)\n",
    "        bmk_score = metrics.mean_absolute_error(y, y_benchmark)\n",
    "        bmk_scores.append(bmk_score)\n",
    "        \n",
    "        predicts = df_cv.reset_index()[['full_name','target_week', response, espn_response]]\n",
    "        predicts['PREDICTION'] = pd.Series(y_pred)\n",
    "        predicts['ERROR'] = predicts['PREDICTION'] - predicts[response]\n",
    "        predicts['ESPN_ERROR'] = predicts[espn_response] - predicts[response]\n",
    "        predictions.append(predicts)\n",
    "        \n",
    "    return week_nums, scores, bmk_scores, pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:47.774759Z",
     "start_time": "2017-12-14T17:12:41.357137Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction_dfs = {}\n",
    "\n",
    "f, (ax2, ax3, ax4, ax5) = plt.subplots(4,figsize=(12,30), sharex=True)\n",
    "for pos, data, ax in [('QB',QB_df_holdout,ax2),\n",
    "                      ('RB',RB_df_holdout,ax3),\n",
    "                      ('WR',WR_df_holdout,ax4),\n",
    "                      ('TE',TE_df_holdout,ax5)]:\n",
    "    \n",
    "    weeks, scores, bmk_scores, predictions = weekly_regression_predict(data, weeks_predict, \n",
    "                                                                       ests_positions[pos],\n",
    "                                                                       response=RESPONSE_VAR,\n",
    "                                                                       espn_response=BENCHMARK)\n",
    "    predictions['position'] = pos\n",
    "    prediction_dfs[pos] = predictions\n",
    "    \n",
    "    ax.plot(weeks, scores, label=\"Linear Model RMSE\", linewidth=4)\n",
    "    ax.plot(weeks, bmk_scores, label=\"ESPN RMSE\", linewidth=4)\n",
    "    ax.set_xticks(weeks)\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.set_title(pos)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:52.876494Z",
     "start_time": "2017-12-14T17:12:52.865041Z"
    }
   },
   "outputs": [],
   "source": [
    "holdout_results = pd.concat(list(prediction_dfs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:53.919230Z",
     "start_time": "2017-12-14T17:12:53.125369Z"
    }
   },
   "outputs": [],
   "source": [
    "weeks_all = []\n",
    "all_scores = []\n",
    "all_bmk_scores = []\n",
    "for week in weeks_predict:\n",
    "    weeks_all.append(week)\n",
    "    weeks_results = holdout_results[holdout_results.target_week == week]\n",
    "    y = weeks_results[RESPONSE_VAR]\n",
    "    y_pred = weeks_results['PREDICTION']\n",
    "    y_bmk = weeks_results[BENCHMARK]\n",
    "#     score = metrics.r2_score(y, y_pred)\n",
    "#     bmk_score = metrics.r2_score(y, y_bmk)\n",
    "    score = (metrics.mean_squared_error(y, y_pred))**(0.5)\n",
    "    bmk_score = (metrics.mean_squared_error(y, y_bmk))**(0.5)\n",
    "    all_scores.append(score)\n",
    "    all_bmk_scores.append(bmk_score)\n",
    "    \n",
    "print(np.mean(all_scores))\n",
    "print(np.mean(all_bmk_scores))\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.plot(weeks_all, all_scores, label=\"Linear Model\", linewidth=4)\n",
    "plt.plot(weeks_all, all_bmk_scores, label=\"ESPN\", linewidth=4)\n",
    "plt.xticks(weeks_all)\n",
    "# plt.ylabel(\"RMSE\")\n",
    "# plt.xlabel(\"Week\")\n",
    "# plt.title(\"Holdout error for all positions (2017 season)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:57.227944Z",
     "start_time": "2017-12-14T17:12:57.176888Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "position_q = 'RB'\n",
    "name = 'LeSean McCoy'\n",
    "holdout_results[(holdout_results.position == position_q) &\n",
    "                (holdout_results.full_name == name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:57.894691Z",
     "start_time": "2017-12-14T17:12:57.875688Z"
    }
   },
   "outputs": [],
   "source": [
    "holdout_results['abs_error'] = holdout_results.ERROR.apply(lambda x: np.abs(x))\n",
    "holdout_results['spread'] = np.abs(holdout_results['ERROR'] - holdout_results['ESPN_ERROR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-14T17:12:58.883464Z",
     "start_time": "2017-12-14T17:12:58.823445Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "holdout_results.sort_values('spread', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
